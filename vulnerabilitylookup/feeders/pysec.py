#!/usr/bin/env python3

from __future__ import annotations

import re

from datetime import datetime
import orjson
from pathlib import Path

import yaml

from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class PySec(AbstractFeeder):
    def __init__(self) -> None:
        super().__init__(Path(__file__).stem)

        self.init_git_repo()

    def update(self) -> bool:
        self.git.remotes.origin.pull("main")

        paths_to_import: set[Path] = set()
        if _last_update := self.storage.hget("last_updates", self.name):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.git.head.commit.hexsha:
                # No changes
                self.logger.info("No new commit.")
                return False
            for commit in self.git.iter_commits(f"{_last_update_str}...HEAD"):
                for line in self.git.git.show(commit.hexsha, name_only=True).split("\n"):
                    if not line.endswith(".yaml"):
                        continue
                    p_path = self.path_to_repo / Path(line)
                    if p_path.exists() and re.match(r"PYSEC-\d{4}-\d+.yaml", p_path.name):
                        paths_to_import.add(p_path)
        else:
            # First run, get all files
            for p_path in self.path_to_repo.rglob("*.yaml"):
                if p_path.exists() and re.match(r"PYSEC-\d{4}-\d+.yaml", p_path.name):
                    paths_to_import.add(p_path)

        if not paths_to_import:
            self.logger.info("Nothing new to import.")
            return False

        p = self.storage.pipeline()
        pysecids: dict[str, float] = {}
        for path in paths_to_import:
            needs_lastmodified_from_git = False
            last_modified = None
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln = yaml.safe_load(vuln_entry)
                if not isinstance(vuln["modified"], datetime):
                    last_modified = fromisoformat_wrapper(vuln["modified"])
                else:
                    last_modified = vuln["modified"]

                if not last_modified:
                    if not needs_lastmodified_from_git:
                        self.logger.warning(f"Unable to process {path}, please have a look yourself, good luck!")
                        continue

                    # NOTE old approach: there is no indication when the entry was last updated in the json,
                    # using the last time that file was commited
                    # It is slow as hell, but that's the best we can do.

                    commit = next(self.git.iter_commits(max_count=1, paths=path))
                    last_modified = commit.committed_datetime
                vuln_id = vuln["id"].lower()
                pysecids[vuln_id] = last_modified.timestamp()
                if "aliases" in vuln:
                    for alias in vuln["aliases"]:
                        a = alias.lower()
                        p.sadd(f"{vuln_id}:link", a)
                        p.sadd(f"{a}:link", vuln_id)
                p.set(vuln_id, orjson.dumps(vuln))

            if len(pysecids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f"index:{self.name}", pysecids)  # type: ignore
                p.zadd("index", pysecids)  # type: ignore
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                pysecids = {}

        if pysecids:
            # remaining entries
            p.zadd(f"index:{self.name}", pysecids)  # type: ignore
            p.zadd("index", pysecids)  # type: ignore
            p.execute()
        self.storage.hset("last_updates", mapping={self.name: self.git.head.commit.hexsha})
        self.logger.info("Import done.")
        return True
