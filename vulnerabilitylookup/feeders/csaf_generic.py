from __future__ import annotations

import os
import re
import time

import orjson
import requests

from pathlib import Path
from subprocess import Popen

from ..default import get_config, get_homedir
from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class CSAFGeneric(AbstractFeeder):
    def __init__(self, feeder_name: str) -> None:
        super().__init__(feeder_name)

        csaf_downloader = get_config("generic", "csaf_downloader_path")
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error("A CSAF downloader is required for this feeder")
            raise FileNotFoundError("CSAF downloader not found")

        if not hasattr(self, "csaf_metadata"):
            self.logger.error("The URL to the metadata file is required for this feeder")
            raise Exception("Link to the metadata CSAF repository is missing.")

        if not hasattr(self, "file_pattern"):
            self.file_pattern = ".*json$"
        self.init_csaf_repo()

    def init_csaf_repo(self) -> None:
        csaf_downloader = get_config("generic", "csaf_downloader_path")
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error("A CSAF downloader is required for this feeder")
            raise FileNotFoundError("CSAF downloader not found")

        self.path_to_repo = get_homedir() / "vulnerabilitylookup" / "feeders" / self.name
        if not self.path_to_repo.exists():
            self.pull_csaf_repo()

    def pull_csaf_repo(self, last_update: str | None = None) -> None:
        csaf_downloader = get_config("generic", "csaf_downloader_path")
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error("A CSAF downloader is required for this feeder")
            raise FileNotFoundError("CSAF downloader not found")

        if not hasattr(self, "csaf_metadata"):
            self.logger.error("The URL to the metadata file is required for this feeder")
            raise Exception("Link to the metadata CSAF repository is missing.")

        to_run = [csaf_downloader, "-d", self.path_to_repo]
        if last_update:
            to_run.append("-t")
            to_run.append(last_update)
        to_run.append(self.csaf_metadata)
        with Popen(to_run) as process:
            self.logger.info("Downloading CSAF data.")
            while process.poll() is None:
                if last_update:
                    self.logger.info(f"Downloading CSAF data since {last_update}")
                else:
                    self.logger.info("Still downloading CSAF data...")
                time.sleep(10)
            self.logger.info("Downloading CSAF data finished")

    def _get_last_update_from_meta(self) -> str:
        if not hasattr(self, "csaf_metadata"):
            self.logger.error("The URL to the metadata file is required for this feeder")
            raise Exception("Link to the metadata CSAF repository is missing.")

        metadata = requests.get(self.csaf_metadata).json()
        return metadata["last_updated"]

    def update(self) -> bool:
        # TODO: try to only get the updates. Not sure it is possible
        # but this link might help: https://github.com/csaf-poc/csaf_distribution/blob/main/docs/csaf_downloader.md#timerange-option
        self.init_csaf_repo()
        last_update = self._get_last_update_from_meta()

        paths_to_import: set[Path] = set()
        if _last_update := self.storage.hget("last_updates", self.name):
            _last_update_str = _last_update.decode()
            # Get last time the repo was updated
            if _last_update_str == last_update:
                # No changes
                self.logger.info("No updates.")
                return False

            self.pull_csaf_repo(_last_update.decode())

            # TODO: only load updated files

        paths_to_import = set()
        for tlp_color in self.path_to_repo.iterdir():
            if not tlp_color.is_dir():
                # That's the logfile
                continue
            for year in tlp_color.iterdir():
                for csaf_file in year.iterdir():
                    if re.match(self.file_pattern, csaf_file.name):
                        paths_to_import.add(csaf_file)

        if not paths_to_import:
            return False

        p = self.storage.pipeline()
        csafids: dict[str, float] = {}
        for path in paths_to_import:
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln = orjson.loads(vuln_entry.read())
                modified = fromisoformat_wrapper(vuln["document"]["tracking"]["current_release_date"])
                vuln_id = path.stem.lower()
                csafids[vuln_id] = modified.timestamp()
                if "vulnerabilities" in vuln and vuln.get("vulnerabilities"):
                    for _v in vuln.get("vulnerabilities"):
                        if cve := _v.get("cve"):
                            cve = cve.lower()
                            p.sadd(f"{vuln_id}:link", cve)
                            p.sadd(f"{cve}:link", vuln_id)
                p.set(vuln_id, orjson.dumps(vuln))
            if len(csafids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f"index:{self.name}", csafids)  # type: ignore
                p.zadd("index", csafids)  # type: ignore
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                csafids = {}

        if csafids:
            # remaining entries
            p.zadd(f"index:{self.name}", csafids)  # type: ignore
            p.zadd("index", csafids)  # type: ignore
            p.execute()
        self.storage.hset("last_updates", mapping={self.name: last_update})
        self.logger.info("Import done.")
        return True
